{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbb4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix, from_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "import pandas as pd\n",
    "import scipy.special as SS\n",
    "import scipy.stats as SSA\n",
    "import copy\n",
    "import math\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "import numpy.linalg as LA\n",
    "import gzip\n",
    "from scipy import sparse\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "\n",
    "# load pickle module\n",
    "import pickle\n",
    "import networkx as nx\n",
    "# from tqdm import tqdm\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6245c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):  # the simpliest model that GNN and it is classical, used as baseline\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.conv3 = GCNConv(64, 16)\n",
    "        self.conv4 = GCNConv(16, 8)\n",
    "        self.fc = torch.nn.Linear(8, 1)\n",
    "\n",
    "    \"\"\"\n",
    "        hyperparameters:\n",
    "        - number of hidden layers\n",
    "        - number of hidden channels\n",
    "        - dropout rate (now it's zero)\n",
    "        - learning rate <- most important to tune\n",
    "        - weight decay\n",
    "        - etc etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc23a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, myloader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    total = 0\n",
    "    results = []\n",
    "    for data in myloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        label = data.y.to(device)\n",
    "        y_true.append(label)\n",
    "        loss = F.mse_loss(output, label)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        results.append(output)\n",
    "\n",
    "    return loss_all / len(myloader.dataset), results, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a168f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            # Assume you have a loss function defined, e.g., MSE for regression\n",
    "            loss = F.mse_loss(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1ac897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    true_values = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.mse_loss(output, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            predictions.append(output.cpu())\n",
    "            true_values.append(data.y.cpu())\n",
    "    return torch.cat(predictions, dim=0), torch.cat(true_values, dim=0), total_loss / len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770f0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78476d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_idx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24680423",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "all_data_len = len(dataset)\n",
    "train_size = int(all_data_len * 0.6)\n",
    "val_size = int(all_data_len * 0.2)\n",
    "test_size = all_data_len - train_size - val_size\n",
    "\n",
    "# train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_data1, val_data1, test_data1 = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "251da0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../codes/statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5396e587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_datasets(train_data1, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "961fd6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3142, 7], edge_index=[2, 10378], edge_attr=[10378, 1], y=[1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5437fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "0 4.119335630204943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 39\u001b[0m     loss, myres, reals \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, device)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# loss_ep.append(loss)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, myloader, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m y_true\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(output, label)\n\u001b[0;32m---> 15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m loss_all \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_graphs \u001b[38;5;241m*\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataset = torch.load('/Users/qingyao/Documents/branching_data/gnn_regression/dataset_7.pt')\n",
    "\n",
    "#     # WN = np.loadtxt('W_avg.csv')\n",
    "# lr_list = [np.power(0.5, i) for i in range(2, 16, 2)]*10\n",
    "# my_lr = lr_list[es_idx]\n",
    "\n",
    "\n",
    "\n",
    "    # Now we can create a DataLoader\n",
    "# train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "# Create a model and an optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(num_node_features=num_x).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=my_lr, weight_decay=5e-4)\n",
    "\n",
    "# training and validation\n",
    "counter = 0\n",
    "count_epochs = 0\n",
    "best = float(\"inf\")\n",
    "epochs = 100\n",
    "patience = 10\n",
    "loss_ep = []\n",
    "# define the random seed\n",
    "\n",
    "seed = 1 #int(time.time()) + es_idx\n",
    "# random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "    # random_float = random.uniform(0, 1)\n",
    "    # torch.manual_seed(es_idx)\n",
    "print('start training')\n",
    "for epoch in range(epochs):\n",
    "    loss, myres, reals = train(model, train_loader, optimizer, device)\n",
    "    # loss_ep.append(loss)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    loss_ep.append(val_loss)\n",
    "    print(epoch,val_loss)\n",
    "    if val_loss < best:\n",
    "        best = val_loss\n",
    "        counter = 0\n",
    "#         torch.save(model.state_dict(), '/rds/general/user/qy1815/home/branching_superspreading/regression_{}/best_model_{}_{}.pth'.format(\n",
    "#             num_x, s, es_idx))  # Save the best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        count_epochs += 1\n",
    "\n",
    "    if counter > patience:\n",
    "        break\n",
    "\n",
    "# testing\n",
    "# test_loader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "# predictions, y_true, test_mse = test(model, test_loader, device)\n",
    "\n",
    "# predictions_np = predictions.cpu().detach().numpy()\n",
    "# y_true_np = y_true.cpu().detach().numpy()\n",
    "\n",
    "# with h5py.File('/rds/general/user/qy1815/home/branching_superspreading/regression_{}/res_{}_{}.hdf5'.format(num_x, s, es_idx), 'w') as f:\n",
    "#     # f['val_mse'] = val_loss\n",
    "#     f.create_dataset('val_mse', data=np.array(loss_ep))\n",
    "#     f.create_dataset('predictions', data=predictions_np)\n",
    "#     f.create_dataset('y_true', data=y_true_np)\n",
    "#     f['test_mse'] = test_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start training\n",
    "0 4.239625602298313"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

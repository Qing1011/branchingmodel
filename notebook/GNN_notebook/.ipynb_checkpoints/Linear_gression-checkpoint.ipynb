{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045a21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, NNConv, global_mean_pool, TopKPooling\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat, savemat\n",
    "import pandas as pd\n",
    "import scipy.special as SS\n",
    "import scipy.stats as SSA\n",
    "import copy\n",
    "import math\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "import numpy.linalg as LA\n",
    "import gzip\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# load pickle module\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d594d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = 2.5\n",
    "rs = np.array([20, 10, 2.0, 1.0, 0.5, 0.2, 0.1, 0.05, 0.025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6409e12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2a49e2a50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(9),np.log(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56b6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "WN = np.loadtxt('../W_avg.csv') ### numpy arrary\n",
    "WN2 = np.dot(WN, WN)\n",
    "# prob = WN[:,1858]\n",
    "prob = WN2[:,1858]\n",
    "prob_2d = prob.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fbbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_s = nx.from_numpy_array(WN)\n",
    "edges = np.array(g_s.edges()).transpose()\n",
    "edge_index = torch.tensor(edges,dtype = torch.int64)\n",
    "# edge_weight = torch.tensor(WN[edge_index[0], edge_index[1]], dtype=torch.float)\n",
    "edge_weights = []\n",
    "for (u, v) in g_s.edges():\n",
    "    edge_weights.append([g_s[u][v]['weight']])\n",
    "edge_weights = torch.tensor(edge_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc84482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gzipped_numpy(filename):\n",
    "    try:\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            return np.load(f, allow_pickle=True)\n",
    "    except FileNotFoundError:\n",
    "        return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0087b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0 0\n",
      "10.0 0\n",
      "2.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.2 2\n",
      "0.1 2\n",
      "0.05 3\n",
      "0.025 3\n"
     ]
    }
   ],
   "source": [
    "export_dir = '/Users/qingyao/Documents/branching_data/simulation/python_cutoff_addno/'\n",
    "#### data preparation\n",
    "dataset = []\n",
    "r_class = {20:0, 10:0, 2.0:1, 1.0:1, 0.5:1, 0.2:2, 0.1:2, 0.05:3, 0.025:3}\n",
    "for r_idx in range(9):\n",
    "    r = rs[r_idx]\n",
    "    sub_export_dir = export_dir+'branching_R0-{}_r-{}/'.format(np.round(R0,2),np.round(r,3))\n",
    "    r_c = r_class[r]\n",
    "    print(r, r_c)\n",
    "    # Create a list to hold our Data objects\n",
    "    for g_idx in range(300):\n",
    "        export_names = sub_export_dir+'NewInf_R0-{}_r-{}_{}.npy.gz'.format(np.round(R0,2),np.round(r,3),(g_idx+1))\n",
    "        g_i = load_gzipped_numpy(export_names)\n",
    "#         g_i_new = np.hstack((g_i[:,10:], prob_2d))\n",
    "        g_i_new = g_i[:,10:] * prob_2d\n",
    "        matrix = torch.from_numpy(g_i_new)\n",
    "        \n",
    "        y = torch.log(torch.tensor([[r]], dtype=torch.float))\n",
    "        # Create a Data object for each graph\n",
    "        data = Data(x=matrix, edge_index=edge_index, edge_attr=edge_weights,y=y)\n",
    "        data.x = data.x.float()\n",
    "#         data.y = data.y.long()\n",
    "        # Add the Data object to our list\n",
    "        dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3079dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_data, test_data = random_split(dataset, [int(2700*0.8), int(2700*0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41143989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qingyao/anaconda3/envs/gnn/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Epoch: 001, Loss: 4.8784:   1%|          | 1/100 [00:10<17:26, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 4.87842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 4.7259:   2%|▏         | 2/100 [00:21<17:32, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 4.72593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 4.4436:   3%|▎         | 3/100 [00:32<17:30, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 4.44360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 3.8233:   4%|▍         | 4/100 [00:43<17:17, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 3.82334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 2.6957:   5%|▌         | 5/100 [00:53<17:07, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 2.69570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 2.1917:   6%|▌         | 6/100 [01:04<16:46, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 2.19166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 1.7979:   7%|▋         | 7/100 [01:15<16:31, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 1.79794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 1.6301:   8%|▊         | 8/100 [01:25<16:17, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 1.63013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 1.2478:   9%|▉         | 9/100 [01:36<16:05, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 1.24782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.2407:  10%|█         | 10/100 [01:46<15:52, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.24067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss: 1.1191:  11%|█         | 11/100 [01:57<15:38, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss: 1.11907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss: 1.2802:  12%|█▏        | 12/100 [02:07<15:27, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss: 1.28016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 0.9907:  13%|█▎        | 13/100 [02:18<15:21, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 0.99071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss: 1.0325:  14%|█▍        | 14/100 [02:29<15:13, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss: 1.03248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 0.8131:  15%|█▌        | 15/100 [02:40<15:12, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 0.81315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss: 1.0143:  16%|█▌        | 16/100 [02:50<14:55, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss: 1.01433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss: 0.8297:  17%|█▋        | 17/100 [03:01<14:40, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss: 0.82969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss: 0.7613:  18%|█▊        | 18/100 [03:11<14:28, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss: 0.76129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss: 0.8455:  19%|█▉        | 19/100 [03:22<14:15, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss: 0.84548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.8909:  20%|██        | 20/100 [03:32<14:07, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.89086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Loss: 0.8306:  21%|██        | 21/100 [03:43<13:57, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Loss: 0.83057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Loss: 0.8562:  22%|██▏       | 22/100 [03:53<13:43, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Loss: 0.85615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Loss: 1.1019:  23%|██▎       | 23/100 [04:04<13:36, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Loss: 1.10192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Loss: 0.7871:  24%|██▍       | 24/100 [04:15<13:31, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Loss: 0.78711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 0.8897:  25%|██▌       | 25/100 [04:26<13:24, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 0.88974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Loss: 0.7305:  26%|██▌       | 26/100 [04:37<13:14, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Loss: 0.73050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Loss: 0.9704:  27%|██▋       | 27/100 [04:47<13:04, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Loss: 0.97042\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module): ### the simpliest model that GNN and it is classical, used as baseline\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)     \n",
    "        self.conv2 = GCNConv(64, 32) \n",
    "        self.conv3 = GCNConv(32, 16) \n",
    "        self.conv4 = GCNConv(16, 8) \n",
    "        self.classifier = Linear(8, num_classes)\n",
    "\n",
    "    \"\"\"\n",
    "        hyperparameters:\n",
    "        - number of hidden layers\n",
    "        - number of hidden channels\n",
    "        - dropout rate (now it's zero)\n",
    "        - learning rate <- most important to tune\n",
    "        - weight decay\n",
    "        - etc etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        edge_index, edge_weight = add_self_loops(edge_index, edge_attr=edge_weight, num_nodes=x.size(0))\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Now we can create a DataLoader\n",
    "myloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "# Create a model and an optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(num_node_features=50, num_classes=1).to(device) ### only look at the last 30 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    total = 0\n",
    "    results = []\n",
    "    for data in myloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data) \n",
    "        label = data.y.to(device)\n",
    "        y_true.append(label)\n",
    "#         loss = F.cross_entropy(output, label)\n",
    "        loss = F.mse_loss(output, label) \n",
    "        loss.backward() \n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step() \n",
    "        \n",
    "        results.append(output)\n",
    "        \n",
    "        _, predicted = torch.max(output, 1)  \n",
    "#         total += label.size(0)  \n",
    "#         correct += (predicted == label).sum().item()  \n",
    "#     accuracy = 100 * correct / total\n",
    "    return loss_all / len(myloader.dataset), results, y_true\n",
    "\n",
    "loss_ep = []\n",
    "ac_ep = []\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "counter = 0\n",
    "count_epochs = 0\n",
    "best = float(\"inf\")\n",
    "epochs = 100\n",
    "patience = 10\n",
    " \n",
    "for epoch in (pbar := tqdm(range(1, epochs + 1))):\n",
    "    loss, myres, reals = train(model, optimizer, myloader)\n",
    "    loss_ep.append(loss)\n",
    "#     ac_ep.append(ac)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch, loss))\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        count_epochs += 1\n",
    "        \n",
    "    if counter > patience:\n",
    "        break\n",
    "        # print(f\"Epoch: {​​​​​​epoch:03d}​​​​​​, Loss: {​​​​​​loss:.4f}​​​​​​\")\n",
    "    pbar.set_description(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "print(\"\\n\", \"Stopped early at epoch: \", count_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0cd1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b03a9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    true_values = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.mse_loss(output, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            predictions.append(output.cpu())\n",
    "            true_values.append(data.y.cpu())\n",
    "    return torch.cat(predictions, dim=0), torch.cat(true_values, dim=0),total_loss / len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ead3cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 57.0844\n"
     ]
    }
   ],
   "source": [
    "predictions, y_true, test_mse  = test()\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd1e42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c026c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.1000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0500],\n",
       "        [10.0000],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [ 0.1000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [10.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0500],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.5000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [10.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0500],\n",
       "        [ 0.0250],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [10.0000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 2.0000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 0.0500],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [10.0000],\n",
       "        [20.0000],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [ 2.0000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0500],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.2000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [ 0.2000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [ 0.0500],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [10.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.1000],\n",
       "        [10.0000],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [10.0000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 1.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [ 2.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 0.2000],\n",
       "        [ 0.1000],\n",
       "        [ 2.0000],\n",
       "        [ 0.2000],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [ 2.0000],\n",
       "        [ 0.2000],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [10.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [10.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0500],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0500],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 0.5000],\n",
       "        [10.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.5000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [ 1.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.5000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.2000],\n",
       "        [20.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.1000],\n",
       "        [20.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.2000],\n",
       "        [10.0000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [20.0000],\n",
       "        [10.0000],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [ 0.2000],\n",
       "        [ 0.2000],\n",
       "        [ 0.1000],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.5000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.0500],\n",
       "        [20.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.2000],\n",
       "        [ 2.0000],\n",
       "        [10.0000],\n",
       "        [20.0000],\n",
       "        [ 1.0000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 2.0000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [20.0000],\n",
       "        [20.0000],\n",
       "        [ 1.0000],\n",
       "        [10.0000],\n",
       "        [ 1.0000],\n",
       "        [ 0.0250],\n",
       "        [ 0.1000],\n",
       "        [ 0.1000],\n",
       "        [ 0.0250],\n",
       "        [ 0.0500],\n",
       "        [ 0.0500],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [ 0.2000],\n",
       "        [ 0.2000],\n",
       "        [ 0.0250],\n",
       "        [ 1.0000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.5000],\n",
       "        [20.0000],\n",
       "        [ 0.0500],\n",
       "        [ 0.1000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64b06357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 179359.0595, Accuracy: 0/540 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_test = []\n",
    "    y_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data, label = data.to(device), data.y.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = F.mse_loss(output, label)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            \n",
    "            # Store the scores\n",
    "            y_scores.extend(output.cpu().numpy().tolist())\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = output.max(1)\n",
    "            total += label.size(0)\n",
    "            correct += predicted.eq(label).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Test set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)\")\n",
    "\n",
    "    return avg_loss, accuracy, y_test, y_scores\n",
    "\n",
    "# Assuming you have a test_loader, model, and criterion already defined\n",
    "test_loss, test_accuracy, y_test, y_score = test(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df228d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "n_classes = 4\n",
    "colors = {0:'orange',1:'yellowgreen',2:'lightskyblue',3:'cornflowerblue'}\n",
    "labels = {0:r'NS: $r > 5$',1:r'MS: $r \\in [0.5,5]$',2:r'ES: $r \\in (0.5,0.1]$',3:r'IS: $r<0.1$'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838de0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try linear regression \n",
    "## try without the graph structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
